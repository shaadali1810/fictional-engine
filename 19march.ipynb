{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d4c79f5-f689-426e-a38b-cf960e2f41db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MinMax Scaler is one of the most popular scaling algorithms. It transforms features by scaling each feature to a given range, which is generally [0,1], or [-1,-1] in case of negative values.\\n\\nFor each feature, the MinMax Scaler follows the formula:\\n\\n\\nIt subtracts the mean of the column from each value and then divides by the range, i.e, max(x)-min(x).\\n\\nThis scaling algorithm works very well in cases where the standard deviation is very small, or in cases which don’t have Gaussian distribution.\\n\\nLet’s look at an example of MinMax Scaler in Python. Consider, the following data:\\n\\ndata = [[0,5],[2,13],[-3,7],[1,-4],[6,0]]\\nNow, let’s scale this data using sklearn’s MinMax Scaler:\\n\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\nmms = MinMaxScaler().fit(data)\\n\\nprint(mms.transform(data))'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1\n",
    "'''MinMax Scaler is one of the most popular scaling algorithms. It transforms features by scaling each feature to a given range, which is generally [0,1], or [-1,-1] in case of negative values.\n",
    "\n",
    "For each feature, the MinMax Scaler follows the formula:\n",
    "\n",
    "\n",
    "It subtracts the mean of the column from each value and then divides by the range, i.e, max(x)-min(x).\n",
    "\n",
    "This scaling algorithm works very well in cases where the standard deviation is very small, or in cases which don’t have Gaussian distribution.\n",
    "\n",
    "Let’s look at an example of MinMax Scaler in Python. Consider, the following data:\n",
    "\n",
    "data = [[0,5],[2,13],[-3,7],[1,-4],[6,0]]\n",
    "Now, let’s scale this data using sklearn’s MinMax Scaler:\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler().fit(data)\n",
    "\n",
    "print(mms.transform(data))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d0bbbae-e154-4f2a-97cb-8f4aa4079a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Another option that is widely used in machine-learning is to scale the components of a feature vector such that the complete vector has length one. This usually means dividing each component by the Euclidean length of the vector:'''In some applications (e.g., histogram features) it can be more practical to use the L1 norm (i.e., taxicab geometry) of the feature vector. This is especially important if in the following learning steps the scalar metric is used as a distance measure.[why?] Note that this only works ApplicationIn stochastic gradient descent, feature scaling can sometimes improve the convergence speed of the algorithm.[4] In support vector machines,[5] it can reduce the time to find support vectors. Note that feature scaling changes the SVM result'''\n",
    "#Differences in the scales across input variables may increase the difficulty of the problem being modeled. An example of this is that large input values (e.g. a spread of hundreds or thousands of units) can result in a model that learns large weight values. A model with large weight values is often unstable, meaning that it may suffer from poor performance during learning and sensitivity to input values resulting in higher generalization error.\n",
    "#This difference in scale for input variables does not affect all machine learning algorithms.\n",
    "\n",
    "#For example, algorithms that fit a model that use a weighted sum of input variables are affected, such as linear regression, logistic regression, and artificial neural networks (deep learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce512eaa-f735-4cc9-92f1-8941d05ccc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Principal Component Analysis(PCA) is one of the most popular linear dimension reduction algorithms. It is a projection based method that transforms the data by projecting it onto a set of orthogonal(perpendicular) axes.\\n\\n“PCA works on a condition that while the data in a higher-dimensional space is mapped to data in a lower dimension space, the variance or spread of the data in the lower dimensional space should be maximum.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3\n",
    "'''Principal Component Analysis(PCA) is one of the most popular linear dimension reduction algorithms. It is a projection based method that transforms the data by projecting it onto a set of orthogonal(perpendicular) axes.\n",
    "\n",
    "“PCA works on a condition that while the data in a higher-dimensional space is mapped to data in a lower dimension space, the variance or spread of the data in the lower dimensional space should be maximum.'''\n",
    "#ur new data points will be the projections (red points) of those original blue data points. As we can see we have transformed 2-dimensional data points to one-dimensional data points by projection them on 1-dimensional space i.e. a straight line. That magenta straight line is called the principal axis. Since we are projecting to a single dimension, we have only one principal axis. We apply the same procedure to find the next principal axis from the residual variance. Apart from being the direction of maximum variance, the next principal axis must be orthogonal(perpendicular or Uncorrelated to each other,) to the other principal axes.\n",
    "\n",
    "#Once, we get all the principal axes, the dataset is projected onto these axes. The columns in the projected or transformed dataset are called principal components.\n",
    "\n",
    "#The principal components are essentially the linear combinations of the original variables, the weights vector in this combination is actually the eigenvector found which in turn satisfies the principle of least squares.\n",
    "\n",
    "#Luckily, thanks to linear algebra we don’t have to sweat much for PCA. Eigenvalue Decomposition and Singular Value Decomposition(SVD) from linear algebra are the two main procedures used in PCA to reduce dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c11f504-28d9-47c6-a9f4-d9cb1f74aaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Feature Extraction using PCA – Python Example\\nAugust 10, 2020 by Ajitesh Kumar · 2 Comments\\nTaj Mahal Side View\\nIn this post, you will learn about how to use principal component analysis (PCA) for extracting important features (also termed as feature extraction technique) from a list of given features. As a machine learning / data scientist, it is very important to learn the PCA technique for feature extraction as it helps you visualize the data in the lights of importance of explained variance of data set. The following topics get covered in this post:\\n\\nWhat is principal component analysis?\\nPCA algorithm for feature extraction\\nPCA Python implementation step-by-step\\nPCA Python Sklearn example\\nTable of Contents\\nWhat is Principal Component Analysis?\\nHow is PCA different than other feature selection techniques?\\nPCA Algorithm for Feature Extraction\\nPCA Python Implementation Step-by-Step\\nPCA Python Sklearn Example\\nConclusions\\nWhat is Principal Component Analysis?\\nPrincipal component analysis (PCA) is an unsupervised linear transformation technique which is primarily used for feature extraction and dimensionality reduction. It aims to find the directions of maximum variance in high-dimensional data and projects the data onto a new subspace with equal or fewer dimensions than the original one. In the diagram given below, note the directions of maximum variance of data. This is represented using PCA1 (first maximum variance) and PC2 (2nd maximum variance).'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4\n",
    "'''Feature Extraction using PCA – Python Example\n",
    "August 10, 2020 by Ajitesh Kumar · 2 Comments\n",
    "Taj Mahal Side View\n",
    "In this post, you will learn about how to use principal component analysis (PCA) for extracting important features (also termed as feature extraction technique) from a list of given features. As a machine learning / data scientist, it is very important to learn the PCA technique for feature extraction as it helps you visualize the data in the lights of importance of explained variance of data set. The following topics get covered in this post:\n",
    "\n",
    "What is principal component analysis?\n",
    "PCA algorithm for feature extraction\n",
    "PCA Python implementation step-by-step\n",
    "PCA Python Sklearn example\n",
    "Table of Contents\n",
    "What is Principal Component Analysis?\n",
    "How is PCA different than other feature selection techniques?\n",
    "PCA Algorithm for Feature Extraction\n",
    "PCA Python Implementation Step-by-Step\n",
    "PCA Python Sklearn Example\n",
    "Conclusions\n",
    "What is Principal Component Analysis?\n",
    "Principal component analysis (PCA) is an unsupervised linear transformation technique which is primarily used for feature extraction and dimensionality reduction. It aims to find the directions of maximum variance in high-dimensional data and projects the data onto a new subspace with equal or fewer dimensions than the original one. In the diagram given below, note the directions of maximum variance of data. This is represented using PCA1 (first maximum variance) and PC2 (2nd maximum variance).'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5ce7479-ddbf-475c-b594-d4c84912bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 first i impport seaborn lib then load the dataset then from sklearn import minmax scaler then fit the feature price ,time ,deleivery time finally transform it 2d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d2b9af-daeb-4327-8b08-2f139bcd477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 if the data set has 3 or more attributes then firstly plot the graph between financial data and market trends , now draw a orthogonal line between the graph of financial  data and market trends then plot the data point on that orthogonal line by the use of Pca we easily reduce the dimensionality of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a58d6c-8620-4225-876c-c753518b337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d0deffc-4f8b-49d9-80c6-ab8c6decdc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "328551b3-13b6-4a9b-840d-c4b0c825031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([1,5,10,15,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fec2329b-9331-4440-88a5-a9635cee8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2a01c17-0b34-4912-b448-8076cd9ced8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ec2302d-95f7-4f5d-adf2-6f65bf98d20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.21052632],\n",
       "       [0.47368421],\n",
       "       [0.73684211],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max.fit_transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84592667-5548-4b80-81df-8f8d6f73d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27d74b2e-43b5-4fd1-85dd-ae0ab047e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.DataFrame(['height','weight','age','gender','blood pressure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "675c915d-94b9-4d4d-90e7-a56271c24f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                 0\n",
       "0          height\n",
       "1          weight\n",
       "2             age\n",
       "3          gender\n",
       "4  blood pressure>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea9e793-d581-4d95-a284-caac19b1843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import P"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
